{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyp7dZgC2csY"
      },
      "outputs": [],
      "source": [
        "!pip install \"shapely<2.0.0\"\n",
        "! pip install google-cloud-aiplatform langchain chromadb pydantic typing-inspect typing_extensions pandas datasets google-api-python-client pypdf faiss-cpu transformers config --upgrade --user\n"
      ],
      "id": "Uyp7dZgC2csY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "KIDX4jqCiZOq"
      },
      "id": "KIDX4jqCiZOq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain"
      ],
      "metadata": {
        "id": "KuRs5JKJo1s3"
      },
      "id": "KuRs5JKJo1s3"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "import langchain\n",
        "from langchain.llms import VertexAI\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "PROJECT_ID = \"vertext-ai-dar\"  # @param {type:\"string\"}\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n"
      ],
      "metadata": {
        "id": "fhextIUkrf2n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fhextIUkrf2n"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.8,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "llm(\"Tell me a joke\")"
      ],
      "metadata": {
        "id": "DTt24fTh27sH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DTt24fTh27sH"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = \"List the key dates in the life of Steve Jobs\"\n",
        "\n",
        "prompt_2 = \"\"\"\n",
        "    Classify the following question as one of the following: Computers, Audio-Video, or Appliances.\n",
        "\n",
        "    Question: Show me your TVs.\n",
        "    Answer: Audio-Video\n",
        "\n",
        "    Question: Do you have any good deals on PCs?\n",
        "    Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_3 = \"\"\"\n",
        "    Context: You answer questions about dogs.\n",
        "\n",
        "    If someone asks a questions about cats just return \"Woof, I don't know\"\n",
        "\n",
        "    Q: What is a good breed of dog for kids?\n",
        "    A: Golden Retrieves are nice. There are lots of of good dogs too.\n",
        "\n",
        "    Q: What is the best treat for cats?\n",
        "    A: Woof, I don't know\n",
        "\n",
        "    Q: How can I get my cat to stop attacking my dog?\n",
        "    A:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "llm(prompt_1)"
      ],
      "metadata": {
        "id": "ushK-OAj3DsJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ushK-OAj3DsJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates"
      ],
      "metadata": {
        "id": "XtwB4EUwLKDa"
      },
      "id": "XtwB4EUwLKDa"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Context: You write in the style of {style}.\n",
        "    Write me a {output} about {thing}.\n",
        "    \"\"\"\n",
        ")\n",
        "prompt_template.format(style=\"a pirate\", output=\"poem\", thing=\"COBOL Programming\")"
      ],
      "metadata": {
        "id": "nl3KOi04Mer0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nl3KOi04Mer0"
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt_template.format(style=\"a pirate\", output=\"poem\", thing=\"COBOL Programming\"))"
      ],
      "metadata": {
        "id": "kIN96pKrOTIr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kIN96pKrOTIr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Prompt Template"
      ],
      "metadata": {
        "id": "_gFDmb2PSpZs"
      },
      "id": "_gFDmb2PSpZs"
    },
    {
      "cell_type": "code",
      "source": [
        "def rb(arg):\n",
        "    r = 0\n",
        "    b = arg.bit_length()\n",
        "\n",
        "    for i in range(b):\n",
        "        r <<= 1\n",
        "        r |= (arg & 1)\n",
        "        r >>= 1\n",
        "\n",
        "    return r"
      ],
      "metadata": {
        "id": "dBowPOo-So40"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dBowPOo-So40"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import StringPromptTemplate\n",
        "from pydantic import BaseModel, validator\n",
        "import inspect\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "Given the function name and source code, generate an English language explanation of the function.\n",
        "Function Name: {function_name}\n",
        "Source Code:\n",
        "{source_code}\n",
        "Explanation:\n",
        "\"\"\"\n",
        "\n",
        "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
        "    \"\"\"A custom prompt template that takes in the function name as input,\n",
        "    and formats the prompt template to provide the source code of the function.\"\"\"\n",
        "\n",
        "    @validator(\"input_variables\")\n",
        "    def validate_input_variables(cls, v):\n",
        "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
        "        if len(v) != 1 or \"function_name\" not in v:\n",
        "            raise ValueError(\"function_name must be the only input_variable.\")\n",
        "        return v\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the source code of the function\n",
        "        source_code = inspect.getsource(kwargs[\"function_name\"])\n",
        "\n",
        "        # Generate the prompt to be sent to the language model\n",
        "        prompt = PROMPT.format(\n",
        "            function_name=kwargs[\"function_name\"].__name__, source_code=source_code\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    def _prompt_type(self):\n",
        "        return \"function-explainer\""
      ],
      "metadata": {
        "id": "EnlFOiPFTAnj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EnlFOiPFTAnj"
    },
    {
      "cell_type": "code",
      "source": [
        "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
        "\n",
        "# Generate a prompt for the function \"get_source_code\"\n",
        "prompt = fn_explainer.format(function_name=rb)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "CZS_VkNvTQ-g"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CZS_VkNvTQ-g"
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt)"
      ],
      "metadata": {
        "id": "mDs1nBqFThgi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mDs1nBqFThgi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template Pipelining"
      ],
      "metadata": {
        "id": "VODWLAeUW0ci"
      },
      "id": "VODWLAeUW0ci"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt = (\n",
        "    PromptTemplate.from_template(\"\"\"Tell me a joke about {topic},\n",
        "    make it funny and in {language}\"\"\")\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "chain.invoke({\"topic\": \"COBOL\", \"language\": \"English\"})\n"
      ],
      "metadata": {
        "id": "9SRU2wiMW5va"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9SRU2wiMW5va"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Parser"
      ],
      "metadata": {
        "id": "ZBQ9qC_GZEWC"
      },
      "id": "ZBQ9qC_GZEWC"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"question to set up a joke\")\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
        "\n",
        "    # You can add custom validation logic easily with Pydantic.\n",
        "    @validator(\"setup\")\n",
        "    def question_ends_with_question_mark(cls, field):\n",
        "        if field[-1] != \"?\":\n",
        "            raise ValueError(\"Badly formed question!\")\n",
        "        return field\n",
        "\n",
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = PydanticOutputParser(pydantic_object=Joke)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# And a query intended to prompt a language model to populate the data structure.\n",
        "prompt_and_model = prompt | llm\n",
        "output = prompt_and_model.invoke({\"query\": \"Tell me a joke about Python programming.\"})\n",
        "\n",
        "parser.invoke(output)"
      ],
      "metadata": {
        "id": "cHj9C22jZHsM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cHj9C22jZHsM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's another example, but with a compound typed field.\n",
        "class Actor(BaseModel):\n",
        "    name: str = Field(description=\"name of an actor\")\n",
        "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
        "\n",
        "\n",
        "actor_query = \"Generate the filmography for a random actor.\"\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Actor)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "_input = prompt.format_prompt(query=actor_query)\n",
        "\n",
        "output = llm(_input.to_string())\n",
        "\n",
        "parser.parse(output)"
      ],
      "metadata": {
        "id": "xzejLH-sbGyb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xzejLH-sbGyb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List Parser"
      ],
      "metadata": {
        "id": "Dot-w_1gG4Ns"
      },
      "id": "Dot-w_1gG4Ns"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"List five {subject}, Only list the items with no formatting.\n",
        "    {format_instructions}\"\"\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "_input = prompt.format(subject=\"ice cream flavors\")\n",
        "output = llm(_input)\n",
        "\n",
        "output_parser.parse(output)"
      ],
      "metadata": {
        "id": "lTR-eDcscIGB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lTR-eDcscIGB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's another example, but with a compound typed field.\n",
        "class Athlete(BaseModel):\n",
        "    name: str = Field(description=\"name of an athlete\")\n",
        "    teams: List[str] = Field(description=\"list of teams they played for\")\n",
        "\n",
        "\n",
        "athlete_query = \"Name a random NFL Quarterback from the 1900s.\"\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Athlete)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "_input = prompt.format_prompt(query=athlete_query)\n",
        "\n",
        "output = llm(_input.to_string())\n",
        "\n",
        "parser.parse(output)"
      ],
      "metadata": {
        "id": "ukXYqwK5cIdY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ukXYqwK5cIdY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LCEL"
      ],
      "metadata": {
        "id": "ciLIR8e--xdf"
      },
      "id": "ciLIR8e--xdf"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "prompt = (\n",
        "    PromptTemplate.from_template(\"Write me a poem about {topic}\"\n",
        "    + \", make it rhyme\"\n",
        "    + \"\\n\\nand in {language}\")\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# LCEL Syntax\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "for chunk in chain.stream({\"topic\": \"COBOL\", \"language\": \"English\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "09AIem63-wbl"
      },
      "execution_count": null,
      "outputs": [],
      "id": "09AIem63-wbl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat"
      ],
      "metadata": {
        "id": "X-_3nkx6Mf1v"
      },
      "id": "X-_3nkx6Mf1v"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "chat = ChatVertexAI(model_name=\"chat-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,)\n",
        "\n",
        "chat([HumanMessage(content=\"What's a good recipe for a Halloween Party?\")])"
      ],
      "metadata": {
        "id": "S_c5RdAR44GQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "S_c5RdAR44GQ"
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a bot who knows about cooking\"),\n",
        "        HumanMessage(content=\"What's a good desert for Thanksgiving\"),\n",
        "        AIMessage(content=\"Pumpkin pie is always a winner.\"),\n",
        "        HumanMessage(content=\"Great, what is the recipe?\")\n",
        "    ]\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "DpHLqg0fHLhM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DpHLqg0fHLhM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatPromptTemplate"
      ],
      "metadata": {
        "id": "H4VZiEAKO9EW"
      },
      "id": "H4VZiEAKO9EW"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful {job}. Your name is {name}.\"),\n",
        "        (\"human\", \"Hello, how are you doing?\"),\n",
        "        (\"ai\", \"I'm doing well, thanks!\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "messages = chat_template.format_messages(\n",
        "                      job=\"Chef\",\n",
        "                      name=\"Julia\",\n",
        "                      user_input=\"What is your name and what do you do?\")\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "VRuE-jHEO7CY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "VRuE-jHEO7CY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat with Memory"
      ],
      "metadata": {
        "id": "VX5FX11EFZWp"
      },
      "id": "VX5FX11EFZWp"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=chat, memory=memory, verbose=False)\n",
        "\n",
        "input = \"\"\"\n",
        "    System: You are a Chef named Julia.\n",
        "    Human: What is a good recipe for dinner that includes bananas?\n",
        "\"\"\"\n",
        "\n",
        "conversation.predict(input = input)"
      ],
      "metadata": {
        "id": "G6AMfE10-NUs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "G6AMfE10-NUs"
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"How long would that take to prepare?\")"
      ],
      "metadata": {
        "id": "P-ZouKyPFELj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "P-ZouKyPFELj"
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Should those be served warm or chilled\")"
      ],
      "metadata": {
        "id": "Id4DetKhFju6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Id4DetKhFju6"
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "wLpT3ISsLceq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wLpT3ISsLceq"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(self, texts: List[str]):\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]"
      ],
      "metadata": {
        "id": "EDy2Kptjsx6_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EDy2Kptjsx6_"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(self, texts: List[str]):\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]\n",
        "\n",
        "# Embedding\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH = 5\n",
        "embeddings = CustomVertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        ")"
      ],
      "metadata": {
        "id": "DY8wWTqpsvT0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DY8wWTqpsvT0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}