{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIMTPB1WoTq"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPmcMNgZpo9V"
      },
      "source": [
        "# Analyze a codebase with the Vertex AI Gemini 1.5 Pro\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase_with_gemini_1_5_pro.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fcode%2Fanalyze_codebase_with_gemini_1_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/code/analyze_codebase_with_gemini_1_5_pro.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/code/analyze_codebase_with_gemini_1_5_pro.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EExYZvij2ve"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Eric Dong](https://github.com/gericdong), [Aakash Gouda](https://github.com/aksstar)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yVV6txOmNMn"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Gemini 1.5 Pro introduces a breakthrough long context window of up to 1 million tokens that can help seamlessly analyze, classify and summarize large amounts of content within a given prompt. With its long-context reasoning, Gemini 1.5 Pro can analyze an entire codebase for deeper insights.\n",
        "\n",
        "In this tutorial, you learn how to analyze an entire codebase with Gemini 1.5 Pro and prompt the model to:\n",
        "\n",
        "- **Analyze**: Summarize codebases effortlessly.\n",
        "- **Guide**: Generate clear developer getting-started documentation.\n",
        "- **Debug**: Uncover critical bugs and provide fixes.\n",
        "- **Enhance**: Implement new features and improve reliability and security.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdMGtr18rFdL"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user --quiet google-cloud-aiplatform \\\n",
        "                                        gitpython \\\n",
        "                                        magika"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbozY-XKee95"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSCFmvOWBas9"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import IPython.display\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from vertexai.generative_models import (\n",
        "    FunctionDeclaration,\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    Tool,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNoOTMp2fe33"
      },
      "source": [
        "## Cloning a codebase\n",
        "\n",
        "You will use repo [Online Boutique](https://github.com/GoogleCloudPlatform/microservices-demo) as an example in this notebook. Online Boutique is a cloud-first microservices demo application. The application is a web-based e-commerce app where users can browse items, add them to the cart, and purchase them. This application consists of 11 microservices across multiple languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GlDOs49qgStM"
      },
      "outputs": [],
      "source": [
        "# The GitHub repository URL\n",
        "#repo_url = \"https://github.com/GoogleCloudPlatform/microservices-demo\"  # @param {type:\"string\"}\n",
        "\n",
        "# The location to clone the repo\n",
        "repo_dir = \"/content/drive/MyDrive/genWealth/genwealth\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rm -r genWealth"
      ],
      "metadata": {
        "id": "kczIh7zNIfpF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAm1ly9pfIEX"
      },
      "source": [
        "#### Define helper functions for processing GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "stNia6UaHau2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "import git\n",
        "import magika\n",
        "import requests\n",
        "\n",
        "m = magika.Magika()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_code(repo_dir):\n",
        "    \"\"\"Create an index, extract content of code/text files.\"\"\"\n",
        "\n",
        "    code_index = []\n",
        "    code_text = \"\"\n",
        "    for root, _, files in os.walk(repo_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            relative_path = os.path.relpath(file_path, repo_dir)\n",
        "            code_index.append(relative_path)\n",
        "\n",
        "            file_type = m.identify_path(Path(file_path))\n",
        "            if file_type.output.group in (\"text\", \"code\"):\n",
        "                try:\n",
        "                    with open(file_path) as f:\n",
        "                        code_text += f\"----- File: {relative_path} -----\\n\"\n",
        "                        code_text += f.read()\n",
        "                        code_text += \"\\n-------------------------\\n\"\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    return code_index, code_text\n",
        "\n",
        "\n",
        "def get_github_issue(owner: str, repo: str, issue_number: str) -> str:\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github+json\",\n",
        "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
        "    }  # Set headers for GitHub API\n",
        "\n",
        "    # Construct API URL\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}/issues/{issue_number}\"\n",
        "\n",
        "    try:\n",
        "        response_git = requests.get(url, headers=headers)\n",
        "        response_git.raise_for_status()  # Check for HTTP errors\n",
        "    except requests.exceptions.RequestException as error:\n",
        "        print(f\"Error fetching issue: {error}\")  # Handle potential errors\n",
        "\n",
        "    issue_data = response_git.json()\n",
        "    if issue_data:\n",
        "        return issue_data[\"body\"]\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1UyVuQLuTKE"
      },
      "source": [
        "#### Create an index and extract content of a codebase\n",
        "\n",
        "Clone the repo and create an index and extract content of code/text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rqaYzmNQuTKQ"
      },
      "outputs": [],
      "source": [
        "#clone_repo(repo_url, repo_dir)\n",
        "\n",
        "code_index, code_text = extract_code(repo_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvtwpTxVFZeb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiVQB5SKekS0"
      },
      "source": [
        "## Analyzing the codebase with Gemini 1.5 Pro\n",
        "\n",
        "With its long-context reasoning, Gemini 1.5 Pro can process the codebase and answer questions about the codebase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1nfXrqRxVX"
      },
      "source": [
        "#### Load the Gemini 1.5 Pro model\n",
        "\n",
        "Learn more about the [Gemini API models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vB9gY3WruzK9"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-1.5-pro\"  # @param {type:\"string\"}\n",
        "\n",
        "model = GenerativeModel(\n",
        "    MODEL_ID,\n",
        "    system_instruction=[\n",
        "        \"You are a coding expert.\",\n",
        "        \"Your mission is to answer all code related questions with given context and instructions.\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yedKsUEEvNyb"
      },
      "source": [
        "#### Define a helper function to generate a prompt to a code related question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1DGzMhjCvCpj"
      },
      "outputs": [],
      "source": [
        "def get_code_prompt(question):\n",
        "    \"\"\"Generates a prompt to a code related question.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Questions: {question}\n",
        "\n",
        "    Context:\n",
        "    - The entire codebase is provided below.\n",
        "    - Here is an index of all of the files in the codebase:\n",
        "      \\n\\n{code_index}\\n\\n.\n",
        "    - Then each of the files is concatenated together. You will find all of the code you need:\n",
        "      \\n\\n{code_text}\\n\\n\n",
        "\n",
        "    Answer:\n",
        "  \"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3OtaszvJt9L"
      },
      "source": [
        "### 1. Summarizing the codebase\n",
        "\n",
        "\n",
        "Generate a summary of the codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uMexx1Qtf1ML",
        "outputId": "9720373f-adee-4fc6-8004-1d0272abeb1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer:\n",
            "```sql\n",
            "/*\n",
            "####################################################################################\n",
            "# Copyright 2024 Google LLC\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     https://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "####################################################################################\n",
            "*/\n",
            "\n",
            "\n",
            "-- DESCRIPTION: Example PostgreSQL function to dynamically build a RAG-enriched \n",
            "--              prompt and invoke an LLM. \n",
            "-- DISCLAIMER:  This function is provided for demonstration purposes only and\n",
            "--              should not be used in production without sufficient testing.\n",
            "-- EXAMPLE USAGE:  SELECT * FROM llm(prompt => 'This is a simple prompt.');\n",
            "DROP FUNCTION IF EXISTS llm;\n",
            "CREATE FUNCTION llm(\n",
            "    set_debug BOOLEAN DEFAULT false,\n",
            "    enable_history BOOLEAN DEFAULT false,\n",
            "    enable_stock_lookup BOOLEAN DEFAULT false,\n",
            "    uid INT DEFAULT 2147483647,\n",
            "    model TEXT DEFAULT 'text-bison@002',\n",
            "    user_role TEXT DEFAULT 'I am a generic user',\n",
            "    llm_role TEXT DEFAULT ' You are a helpful AI Assistant',\n",
            "    mission TEXT DEFAULT null,\n",
            "    additional_context TEXT DEFAULT null,\n",
            "    output_format TEXT DEFAULT null,\n",
            "    examples TEXT DEFAULT null,\n",
            "    prompt TEXT DEFAULT 'Tell me I need to pass in a prompt parameter.',\n",
            "    output_instructions TEXT DEFAULT null,\n",
            "    response_restrictions TEXT DEFAULT 'You have no response restrictions for this prompt.',\n",
            "    disclaimer TEXT DEFAULT null,\n",
            "    max_output_tokens INT DEFAULT 512,\n",
            "    temperature DECIMAL DEFAULT 0.0,\n",
            "    top_p DECIMAL DEFAULT 0.95,\n",
            "    top_k DECIMAL DEFAULT 40\n",
            ")\n",
            "RETURNS TABLE (\n",
            "    llm_prompt TEXT,\n",
            "    llm_prompt_len INT,\n",
            "    llm_response TEXT,\n",
            "    llm_response_len INT,\n",
            "    extractive_prompt TEXT,\n",
            "    extractive_response TEXT,\n",
            "    recommended_tickers TEXT\n",
            ")\n",
            "LANGUAGE plpgsql\n",
            "AS $$\n",
            "DECLARE\n",
            "    llm_prompt TEXT := '';\n",
            "    llm_prompt_len INT := null;\n",
            "    llm_response TEXT := null;\n",
            "    interaction_history_count INT := 0;\n",
            "    extractive_prompt TEXT := '';\n",
            "    extractive_response TEXT := '';\n",
            "    recommended_tickers TEXT := '';\n",
            "BEGIN\n",
            "    -- Define user and AI roles\n",
            "    IF llm_role IS NOT null THEN SELECT CONCAT(llm_prompt, 'AI ROLE: ', llm_role, E'.\\n') INTO llm_prompt; END IF;\n",
            "    IF mission IS NOT null THEN SELECT CONCAT(llm_prompt, 'AI MISSION: ', mission, E'.\\n') INTO llm_prompt; END IF;\n",
            "    IF user_role IS NOT null THEN SELECT CONCAT(llm_prompt, 'USER ROLE: ', user_role, E' \\n\\n') INTO llm_prompt; END IF;\n",
            "    \n",
            "    -- Define the task/prompt\n",
            "    IF prompt IS NOT null THEN SELECT CONCAT(llm_prompt, E'INSTRUCTIONS: \\n- Respond to the PROMPT using FEWER than ', CAST (ROUND(max_output_tokens * 3) AS TEXT), E' characters, including white space. The PROMPT begins with \"<PROMPT>\" and ends with \"</PROMPT>\". \\n- Use available CONTEXT to improve your response, and tell the user specifically which CONTEXT you used in plain language (do not use programming markup or tags). The context begins with \"<CONTEXT>\" and ends with \"</CONTEXT>\". \\n- Strictly comply with all response restrictions. Response restrictions start with <RESPONSE_RESTRICTIONS> and end with </RESPONSE_RESTRICTIONS>. \\n\\n<PROMPT>\\n ', prompt, E'\\n</PROMPT>\\n\\n') INTO llm_prompt; END IF;\n",
            "    \n",
            "    -- Enforce response restrictions\n",
            "    IF response_restrictions IS NOT null THEN SELECT CONCAT(llm_prompt, E'<RESPONSE_RESTRICTIONS>\\n\\n ', response_restrictions, E' \\n\\n</RESPONSE_RESTRICTIONS>\\n\\n') INTO llm_prompt; END IF;\n",
            "    \n",
            "    -- Open the context tag\n",
            "    SELECT CONCAT(llm_prompt, E'<CONTEXT>\\n\\n') INTO llm_prompt;\n",
            "    \n",
            "    -- Add conversation history\n",
            "    IF enable_history is true THEN\n",
            "        -- Check if this is the first interaction from this user\n",
            "        SELECT COUNT(*) FROM conversation_history WHERE user_id = uid INTO interaction_history_count;\n",
            "        \n",
            "        -- Add last interaction to prompt\n",
            "        SELECT CONCAT(llm_prompt, E'<LATEST_INTERACTION>\\n==========\\n',\n",
            "            CASE\n",
            "                WHEN interaction_history_count = 0 THEN E' First interaction\\n'\n",
            "                ELSE (SELECT CONCAT(E'**TIME**\\n', datetime, E'\\n\\n**USER**\\n', user_prompt, E'\\n\\n**AI**\\n', ai_response, E'\\n') FROM conversation_history WHERE user_id = uid ORDER BY datetime DESC LIMIT 1)\n",
            "            END,\n",
            "            E'\\n==========\\n</LATEST_INTERACTION>\\n\\n') INTO llm_prompt;\n",
            "        \n",
            "        -- Add other relevant interaction history to prompt\n",
            "        IF interaction_history_count > 1 THEN\n",
            "            WITH ch AS (\n",
            "                SELECT * FROM conversation_history\n",
            "                WHERE user_id = uid\n",
            "                AND id < (SELECT id FROM conversation_history WHERE user_id = uid ORDER BY datetime DESC LIMIT 1)\n",
            "                ORDER BY user_prompt_embedding <=> embedding('textembedding-gecko@003', prompt)\n",
            "                LIMIT 3\n",
            "            )\n",
            "            SELECT CONCAT(llm_prompt, E'<OTHER_INTERACTION_HISTORY>\\n==========\\n', STRING_AGG(CONCAT(E'**TIME**\\n', datetime, E'\\n\\n**USER**\\n ', user_prompt, E'\\n\\n**AI**\\n', ai_response), E'\\n==========\\n<\\OTHER_INTERACTION_HISTORY>\\n\\n'))\n",
            "            INTO llm_prompt FROM ch;\n",
            "        END IF;\n",
            "    END IF;\n",
            "    \n",
            "    -- Add additional_context and examples\n",
            "    IF additional_context IS NOT null THEN SELECT CONCAT(llm_prompt, E'<ADDITIONAL_CONTEXT> Use the following CONTEXT to respond to the PROMPT, and tell me specifically which pieces of CONTEXT you used to improve your response:\\n\\n ', additional_context, E' </ADDITIONAL_CONTEXT>\\n\\n') INTO llm_prompt; END IF;\n",
            "    IF examples IS NOT null THEN SELECT CONCAT(llm_prompt, E'<EXAMPLES> Use the following EXAMPLES to improve your OUTPUT.\\n==========\\n', examples, E' \\n==========\\n</EXAMPLES>\\n\\n') INTO llm_prompt; END IF;\n",
            "    \n",
            "    -- Add output instructions, format, and length constraints\n",
            "    IF output_instructions IS NOT null THEN SELECT CONCAT(llm_prompt, E'<OUTPUT_INSTRUCTIONS> \\nRe-write your OUTPUT using the following instructions:\\n', output_instructions, E' \\n</OUTPUT_INSTRUCTIONS>\\n\\n') INTO llm_prompt; END IF;\n",
            "    IF output_format IS NOT null THEN SELECT CONCAT(llm_prompt, '<OUTPUT_FORMAT> Complete the TASK using the following OUTPUT FORMAT: ', output_format, E' </OUTPUT_FORMAT>\\n\\n') INTO llm_prompt; END IF;\n",
            "    \n",
            "    -- Do stock lookup if enabled\n",
            "    IF enable_stock_lookup is true THEN\n",
            "        SELECT CONCAT(E'List 3 words that best describe the type of investment this person is looking for based on their QUESTION, RISK_PROFILE, and BIO. \\n\\nQUESTION:\\n', prompt, E'\\n\\nRISK_PROFILE: \\n', risk_profile, E'\\n\\nBIO: \\n', bio) INTO extractive_prompt FROM user_profiles WHERE id = uid;\n",
            "        SELECT public.ml_predict_row(\n",
            "            FORMAT('publishers/google/models/%s', model),\n",
            "                json_build_object('instances',\n",
            "                    json_build_object(\n",
            "                      'prompt', extractive_prompt),\n",
            "                      'parameters',\n",
            "                             json_build_object(\n",
            "                                'maxOutputTokens', max_output_tokens,\n",
            "                                'temperature', temperature,\n",
            "                                'topP', top_p,\n",
            "                                'topK', top_k)\n",
            "                    )\n",
            "                ) ->'predictions'->0->>'content' INTO extractive_response;\n",
            "        \n",
            "        WITH inv AS (\n",
            "            SELECT ticker, analysis\n",
            "            FROM investments\n",
            "            WHERE rating = 'BUY'\n",
            "            ORDER BY analysis_embedding <=> embedding('textembedding-gecko@003',extractive_response)\n",
            "            LIMIT 3\n",
            "        )\n",
            "        SELECT\n",
            "            CONCAT(llm_prompt, E'<SUGGESTED_STOCKS> Recommend these specific stock tickers to me, and tell me why they are a good fit for me based on my BIO and personal details: ', STRING_AGG(ticker, ', '), E'\\n\\nTicker Details: ', STRING_AGG(CONCAT(E'\\n========\\n**STOCK TICKER**: ', ticker, E'\\n\\n', analysis), E'\\n'), E'\\n========\\n</SUGGESTED_STOCKS>\\n\\n'),\n",
            "            CONCAT('Tickers: ', STRING_AGG(ticker, ', '))\n",
            "        FROM inv\n",
            "        INTO llm_prompt, recommended_tickers;\n",
            "    END IF;\n",
            "    \n",
            "    -- Close the context tag\n",
            "    SELECT CONCAT(llm_prompt, E'</CONTEXT>\\n\\n') INTO llm_prompt;\n",
            "    \n",
            "    -- Send enriched prompt to LLM\n",
            "    IF set_debug is false THEN\n",
            "        SELECT public.ml_predict_row(\n",
            "            FORMAT('publishers/google/models/%s', model),\n",
            "                json_build_object('instances',\n",
            "                    json_build_object(\n",
            "                      'prompt', llm_prompt),\n",
            "                      'parameters',\n",
            "                             json_build_object(\n",
            "                                'maxOutputTokens', max_output_tokens,\n",
            "                                'temperature', temperature,\n",
            "                                'topP', top_p,\n",
            "                                'topK', top_k)\n",
            "                    )\n",
            "                ) ->'predictions'->0->>'content' INTO llm_response;\n",
            "    END IF;\n",
            "    \n",
            "    -- Record conversation history\n",
            "    IF enable_history is true THEN\n",
            "        INSERT INTO conversation_history (user_id, user_prompt, ai_response)\n",
            "        VALUES (uid, prompt, llm_response);\n",
            "    END IF;\n",
            "    \n",
            "    -- Add disclaimer\n",
            "    IF disclaimer IS NOT null THEN SELECT CONCAT(llm_response, E'\\n\\n', disclaimer) INTO llm_response; END IF;\n",
            "    \n",
            "    -- Return the response\n",
            "    RETURN QUERY SELECT llm_prompt, LENGTH(llm_prompt), llm_response, LENGTH(llm_response), extractive_prompt, extractive_response, recommended_tickers;\n",
            "END;\n",
            "$$;\n",
            "```\n",
            "\n",
            "**Purpose:**\n",
            "\n",
            "The `genwealth-demo_llm.sql` file defines a PostgreSQL function named `llm` that serves as a backend for a Retrieval Augmented Generation (RAG) powered chatbot within the GenWealth application. This function is designed to dynamically construct prompts enriched with user data and application context, then invoke a Large Language Model (LLM) hosted on Vertex AI for text completion.\n",
            "\n",
            "**Detailed Explanation:**\n",
            "\n",
            "1. **Function Definition:**\n",
            "   - The `DROP FUNCTION IF EXISTS llm;` statement ensures that any existing `llm` function is replaced.\n",
            "   - `CREATE FUNCTION llm(...)` defines the function with numerous parameters for controlling its behavior.\n",
            "\n",
            "2. **Parameters:**\n",
            "   - `set_debug`: (Boolean) When `true`, the function will return the constructed prompt without invoking the LLM.\n",
            "   - `enable_history`: (Boolean) When `true`, the function incorporates past conversation history into the prompt.\n",
            "   - `enable_stock_lookup`: (Boolean) When `true`, triggers additional logic to suggest stock tickers relevant to the user's request.\n",
            "   - `uid`: (Integer) The user ID, used for retrieving user-specific information and managing conversation history.\n",
            "   - `model`: (Text) Specifies the LLM model to be used on Vertex AI (default: 'text-bison@002').\n",
            "   - `user_role`, `llm_role`, `mission`: (Text) Define the roles and objectives of the user and the AI in the conversation.\n",
            "   - `additional_context`: (Text) Any extra information to be provided to the LLM, retrieved from the database or elsewhere.\n",
            "   - `output_format`, `examples`: (Text) Guide the format and style of the LLM's response.\n",
            "   - `prompt`: (Text) The user's input question or request.\n",
            "   - `output_instructions`, `response_restrictions`, `disclaimer`: (Text) Control instructions for the LLM's output, limitations on its responses, and a standard legal disclaimer.\n",
            "   - `max_output_tokens`, `temperature`, `top_p`, `top_k`: (Numeric) Parameters for fine-tuning the LLM's response generation.\n",
            "\n",
            "3. **Prompt Construction:**\n",
            "   - The function meticulously builds the `llm_prompt` string using a series of `CONCAT` operations, incorporating the provided parameters and formatting them within specific tags (e.g., `<PROMPT>`, `<CONTEXT>`).\n",
            "   - Conditional logic using `IF` statements determines whether to include certain sections based on the parameter values (e.g., conversation history, stock lookup).\n",
            "\n",
            "4. **Conversation History:**\n",
            "   - If `enable_history` is `true`, the function queries the `conversation_history` table to retrieve past interactions for the user (`uid`).\n",
            "   - The latest interaction and up to three relevant historical interactions are included in the prompt to provide context to the LLM.\n",
            "\n",
            "5. **Stock Lookup:**\n",
            "   - If `enable_stock_lookup` is `true`, the function executes an extractive LLM call using `ml_predict_row` to analyze the user's question, risk profile, and bio to identify relevant investment keywords.\n",
            "   - It then uses these keywords with vector similarity search on the `investments` table to find suitable stock tickers.\n",
            "   - The selected tickers and their details (from the `analysis` column) are added to the prompt.\n",
            "\n",
            "6. **LLM Invocation:**\n",
            "   - If `set_debug` is `false`, the function calls the `ml_predict_row` function to send the constructed `llm_prompt` to the specified LLM model on Vertex AI. \n",
            "   - The LLM's response is captured in `llm_response`.\n",
            "\n",
            "7. **Output and History:**\n",
            "   - The function inserts the user's prompt and the LLM's response into the `conversation_history` table if `enable_history` is `true`.\n",
            "   - It appends the `disclaimer` if provided.\n",
            "   - Finally, it returns a table containing the constructed prompt, its length, the LLM response, its length, and other outputs from the stock lookup process if enabled.\n",
            "\n",
            "**Design Logic:**\n",
            "\n",
            "The function is designed for flexibility and control. It allows users to specify various parameters to tailor the chatbot's behavior and the LLM's responses. The use of SQL enables seamless integration with the existing database and leverages the power of vector similarity search for context retrieval. The structured prompt format with tags ensures clarity and facilitates the LLM's understanding of the user's request and the provided context.\n",
            "\n",
            "\n",
            "Usage metadata:\n",
            "{'prompt_token_count': 335578, 'candidates_token_count': 3550, 'total_token_count': 339128}\n",
            "\n",
            "Finish reason:\n",
            "1\n",
            "\n",
            "Safety settings:\n",
            "[category: HARM_CATEGORY_HATE_SPEECH\n",
            "probability: NEGLIGIBLE\n",
            "probability_score: 0.29296875\n",
            "severity: HARM_SEVERITY_LOW\n",
            "severity_score: 0.251953125\n",
            ", category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
            "probability: NEGLIGIBLE\n",
            "probability_score: 0.287109375\n",
            "severity: HARM_SEVERITY_LOW\n",
            "severity_score: 0.306640625\n",
            ", category: HARM_CATEGORY_HARASSMENT\n",
            "probability: NEGLIGIBLE\n",
            "probability_score: 0.337890625\n",
            "severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "severity_score: 0.1689453125\n",
            ", category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
            "probability: NEGLIGIBLE\n",
            "probability_score: 0.3125\n",
            "severity: HARM_SEVERITY_LOW\n",
            "severity_score: 0.310546875\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "question = \"\"\"\n",
        "  what is the genwealth-demo_llm.sql file doing, explain all the details on whats it doing, why it might be doing and the design logic\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "# Generate text using non-streaming method\n",
        "response = model.generate_content(contents)\n",
        "\n",
        "# Print generated text and usage metadata\n",
        "print(f\"\\nAnswer:\\n{response.text}\")\n",
        "print(f'\\nUsage metadata:\\n{response.to_dict().get(\"usage_metadata\")}')\n",
        "print(f\"\\nFinish reason:\\n{response.candidates[0].finish_reason}\")\n",
        "print(f\"\\nSafety settings:\\n{response.candidates[0].safety_ratings}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JEJoShlFHTOc",
        "outputId": "16494b4d-ac59-49db-d13a-ab3e12af6be7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCshJHPCYoxI"
      },
      "source": [
        "### 2. Creating a developer getting started guide\n",
        "\n",
        "Generate a getting started guide for developers. This sample uses the streaming option to generate the content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6Kns7vCYm1P"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Provide a getting started guide to onboard new developers to the codebase.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXurINu-jelb"
      },
      "source": [
        "### 3. Finding bugs\n",
        "\n",
        "Find the top 3 most severe issues in the codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy3AWPRgNhu_"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Find the top 3 most severe issues in the codebase.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCilrR6FjmfB"
      },
      "source": [
        "### 4. Fixing bug\n",
        "\n",
        "Find the most severe issue in the codebase that can be fixed and provide a code fix for it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwjDh0xGKE2r"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Find the most severe bug in the codebase that you can provide a code fix for.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w2pCULT_xKE"
      },
      "source": [
        "### 5. Implementing a feature request using Function Calling\n",
        "\n",
        "Generate code to implement a feature request.\n",
        "\n",
        "Get feature request text from GitHub Issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMjOy0gJ1_xx"
      },
      "outputs": [],
      "source": [
        "# Function declaration with detailed docstring\n",
        "extract_details_from_url_func = FunctionDeclaration(\n",
        "    name=\"extract_details_from_url\",\n",
        "    description=\"Extracts owner, repository name, and issue number details from a GitHub issue URL\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"owner\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The owner of the GitHub repository.\",\n",
        "            },\n",
        "            \"repo\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The name of the GitHub repository.\",\n",
        "            },\n",
        "            \"issue_number\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The issue number to fetch the body of.\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "# Tool definition\n",
        "extraction_tool = Tool(function_declarations=[extract_details_from_url_func])\n",
        "\n",
        "FEATURE_REQUEST_URL = (\n",
        "    \"https://github.com/GoogleCloudPlatform/microservices-demo/issues/2205\"\n",
        ")\n",
        "\n",
        "# Prompt content\n",
        "prompt_content = f\"What is the feature request of the following {FEATURE_REQUEST_URL}\"\n",
        "\n",
        "# Model generation with tool usage\n",
        "response = model.generate_content(\n",
        "    [prompt_content],\n",
        "    generation_config=GenerationConfig(temperature=0),\n",
        "    tools=[extraction_tool],\n",
        ")\n",
        "# Extract parameters from model response\n",
        "function_call = response.candidates[0].function_calls[0]\n",
        "\n",
        "# Fetch issue details from GitHub API if function call matches\n",
        "if function_call.name == \"extract_details_from_url\":\n",
        "    issue_body = get_github_issue(\n",
        "        function_call.args[\"owner\"],\n",
        "        function_call.args[\"repo\"],\n",
        "        function_call.args[\"issue_number\"],\n",
        "    )\n",
        "\n",
        "IPython.display.Markdown(f\"Feature Request:\\n{issue_body}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19455545f4c9"
      },
      "source": [
        "Use the GitHub Issue text to implement the feature request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78e6df259be8"
      },
      "outputs": [],
      "source": [
        "# Combine feature request with URL and get code prompt\n",
        "question = (\n",
        "    \"Implement the following feature request\" + FEATURE_REQUEST_URL + \"\\n\" + issue_body\n",
        ")\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "\n",
        "# Generate code response\n",
        "response = model.generate_content([prompt])\n",
        "IPython.display.Markdown(response.text)  # Display in Markdown format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOk_Qe35b_cJ"
      },
      "source": [
        "### 6. Creating a troubleshooting guide\n",
        "\n",
        "Create a troubleshooting guide to help resolve common issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKn85LS-v0iw"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "    Provide a troubleshooting guide to help resolve common issues.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h23z0sTsj5pL"
      },
      "source": [
        "### 7. Making the app more reliable\n",
        "\n",
        "Recommend best practices to make the application more reliable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOBSulTPLUAo"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  How can I make this application more reliable? Consider best practices from https://www.r9y.dev/\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf1jNDpJj8u0"
      },
      "source": [
        "### 8. Making the app more secure\n",
        "\n",
        "Recommend best practices to make the application more secure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy_mCyFVLlXU"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  How can you secure the application?\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFfwMOb6kYfw"
      },
      "source": [
        "### 9. Learning the codebase\n",
        "\n",
        "Create a quiz about the concepts used in the codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7jQIUwsNRH4"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Create a quiz about the concepts used in my codebase to help me solidify my understanding.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjo1UrZwGLan"
      },
      "source": [
        "### 10. Creating a quickstart tutorial\n",
        "\n",
        "Create an end-to-end quickstart tutorial for a specific component.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRwmRyDDFRMB"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Please write an end-to-end quickstart tutorial that introduces AlloyDB,\n",
        "  shows how to configure it with the CartService,\n",
        "  and highlights key capabilities of AlloyDB in context of the Online Boutique application.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAJ-kBCZnlH_"
      },
      "source": [
        "### 11. Creating a Git Changelog Generator\n",
        "\n",
        "Understanding changes made between Git commits and highlighting the most important aspects of the changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdeWO8crnlH_"
      },
      "outputs": [],
      "source": [
        "### Fetches commit IDs from a local Git repository on a specified branch.\n",
        "\n",
        "repo = git.Repo(repo_dir)\n",
        "branch_name = \"main\"\n",
        "commit_ids = [\n",
        "    commit.hexsha for commit in repo.iter_commits(branch_name)\n",
        "]  # A list of commit IDs (SHA-1 hashes) in reverse chronological order (newest first)\n",
        "\n",
        "if len(commit_ids) >= 2:\n",
        "    diff_text = repo.git.diff(commit_ids[0], commit_ids[1])\n",
        "\n",
        "    question = \"\"\"\n",
        "      Given the above git diff output, Summarize the important changes made.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = diff_text + question + code_text\n",
        "    contents = [prompt]\n",
        "\n",
        "    responses = model.generate_content(contents, stream=True)\n",
        "    for response in responses:\n",
        "        IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kUeIBfGyoX7"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, you've learned how to use the Gemini 1.5 Pro to analyze a codebase and prompt the model to:\n",
        "\n",
        "- Summarize codebases effortlessly.\n",
        "- Generate clear developer getting-started documentation.\n",
        "- Uncover critical bugs and provide fixes.\n",
        "- Implement new features and improve reliability and security.\n",
        "- Understanding changes made between Git commits"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "analyze_codebase_with_gemini_1_5_pro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}